{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import probplot\n",
    "from scipy.stats.mstats import zscore\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "pd.set_option('max_columns', None)\n",
    "\n",
    "import nltk\n",
    "import collections as co\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read loans.csv as a dataframe\n",
    "loans_df = pd.read_csv('~/Downloads/tanay/data_springboard/loan.csv',low_memory=False, engine='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv',\n",
       "       'term', 'int_rate', 'installment', 'grade', 'sub_grade', 'emp_title',\n",
       "       'emp_length', 'home_ownership', 'annual_inc', 'verification_status',\n",
       "       'issue_d', 'loan_status', 'pymnt_plan', 'url', 'desc', 'purpose',\n",
       "       'title', 'zip_code', 'addr_state', 'dti', 'delinq_2yrs',\n",
       "       'earliest_cr_line', 'inq_last_6mths', 'mths_since_last_delinq',\n",
       "       'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal',\n",
       "       'revol_util', 'total_acc', 'initial_list_status', 'out_prncp',\n",
       "       'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp',\n",
       "       'total_rec_int', 'total_rec_late_fee', 'recoveries',\n",
       "       'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt',\n",
       "       'next_pymnt_d', 'last_credit_pull_d', 'collections_12_mths_ex_med',\n",
       "       'mths_since_last_major_derog', 'policy_code', 'application_type',\n",
       "       'annual_inc_joint', 'dti_joint', 'verification_status_joint',\n",
       "       'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'open_acc_6m',\n",
       "       'open_il_6m', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il',\n",
       "       'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc',\n",
       "       'all_util', 'total_rev_hi_lim', 'inq_fi', 'total_cu_tl',\n",
       "       'inq_last_12m'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to classify loan status into one of the following bins ('Fully Paid', 'Default', 'Current')\n",
    "def loan_status_bin(text):\n",
    "    if text in ('Fully Paid', 'Does not meet the credit policy. Status:Fully Paid'):\n",
    "        return 'Fully Paid'\n",
    "    elif text in ('Current', 'Issued'):\n",
    "        return 'Current'\n",
    "    elif text in ('Charged Off', 'Default', 'Does not meet the credit policy. Status:Charged Off'):\n",
    "        return 'Default'\n",
    "    elif text in ('Late (16-30 days)', 'Late (31-120 days)', 'In Grace Period'):\n",
    "        return 'Late'\n",
    "    else:\n",
    "        'UNKNOWN BIN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Fully Paid', 'Default', 'Current', 'Late'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a new attribute 'loan_status_bin' in the dataframe\n",
    "loans_df['loan_status_bin']=loans_df['loan_status'].apply(loan_status_bin)\n",
    "loans_df['loan_status_bin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df.fillna(loans_df.median()['annual_inc'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_df[loans_df['annual_inc'].isnull()==True]['annual_inc'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df_fp=loans_df[loans_df['loan_status_bin']=='Fully Paid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df_def=loans_df[loans_df['loan_status_bin']=='Default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Default loans, mean annual income is 65199.76680867284, standard deviation is 56955.15545104668, size of dataframe is 47228\n"
     ]
    }
   ],
   "source": [
    "print('For Default loans, mean annual income is {0}, standard deviation is {1}, size of dataframe is {2}'.format(loans_df_def['annual_inc'].mean(), loans_df_def['annual_inc'].std(), len(loans_df_def['annual_inc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Fully Paid loans, mean annual income is 74142.5024192341, standard deviation is 59205.29202398379, size of dataframe is 209711\n"
     ]
    }
   ],
   "source": [
    "print('For Fully Paid loans, mean annual income is {0}, standard deviation is {1}, size of dataframe is {2}'.format(loans_df_fp['annual_inc'].mean(), loans_df_fp['annual_inc'].std(), len(loans_df_fp['annual_inc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_mean=loans_df_def['annual_inc'].mean()\n",
    "def_std=loans_df_def['annual_inc'].std()\n",
    "\n",
    "fp_mean=loans_df_fp['annual_inc'].mean()\n",
    "fp_std=loans_df_fp['annual_inc'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8942.7356105612562, 292.23360521799054)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0_mean = 0\n",
    "mean_diff = abs(def_mean-fp_mean)\n",
    "sigma_diff = np.sqrt((fp_std**2)/len(loans_df_fp) + (def_std**2)/len(loans_df_def))\n",
    "mean_diff, sigma_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.601325278420518"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = (mean_diff - h0_mean) / sigma_diff\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = (1-stats.norm.cdf(z))*2\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to convert grade into numerical values\n",
    "def credit_grade(grade):\n",
    "    if grade in ('A'):\n",
    "        return 1\n",
    "    elif grade in ('B'):\n",
    "        return 2\n",
    "    elif grade in ('C'):\n",
    "        return 3\n",
    "    elif grade in ('D'):\n",
    "        return 4\n",
    "    elif grade in ('E'):\n",
    "        return 5\n",
    "    elif grade in ('F'):\n",
    "        return 6\n",
    "    elif grade in ('G'):\n",
    "        return 7\n",
    "    else:\n",
    "        99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, 5, 6, 4, 7])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a new attribute 'loan_status_bin' in the dataframe\n",
    "loans_df['credit_grade']=loans_df['grade'].apply(credit_grade)\n",
    "loans_df['credit_grade'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['INDIVIDUAL', 'JOINT'], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_df['application_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derived_income(x, y, z):\n",
    "    if x == 'INDIVIDUAL':\n",
    "        return y\n",
    "    elif x == 'JOINT':\n",
    "        return z\n",
    "    else:\n",
    "        0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df['derived_income']=loans_df.apply(lambda x: derived_income(x['application_type'], x['annual_inc'], x['annual_inc_joint']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derived_dti(x, y, z):\n",
    "    if x == 'INDIVIDUAL':\n",
    "        return y\n",
    "    elif x == 'JOINT':\n",
    "        return z\n",
    "    else:\n",
    "        0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df['derived_dti']=loans_df.apply(lambda x: derived_dti(x['application_type'], x['dti'], x['dti_joint']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df['inst_inc_ratio']=loans_df['installment']/ (loans_df['derived_income'] /12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features: \n",
    "* loan_amount\n",
    "* credit_grade \n",
    "* interest_rate \n",
    "* derived_inc\n",
    "* derived_dti \n",
    "* inst_inc_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Datasets\n",
    "\n",
    "When fitting models, we would like to ensure two things:\n",
    "\n",
    "* We have found the best model (in terms of model parameters).\n",
    "* The model is highly likely to generalize i.e. perform well on unseen data.\n",
    "\n",
    "<br/>\n",
    "<div class=\"span5 alert alert-success\">\n",
    "<h4>Purpose of splitting data into Training/testing sets</h4>\n",
    "<ul>\n",
    "  <li> We built our model with the requirement that the model fit the data well. </li>\n",
    "  <li> As a side-effect, the model will fit <b>THIS</b> dataset well. What about new data? </li>\n",
    "    <ul>\n",
    "      <li> We wanted the model for predictions, right?</li>\n",
    "    </ul>\n",
    "  <li> One simple solution, leave out some data (for <b>testing</b>) and <b>train</b> the model on the rest </li>\n",
    "  <li> This also leads directly to the idea of cross-validation, next section. </li>  \n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "First, we try a basic Logistic Regression:\n",
    "\n",
    "* Split the data into a training and test (hold-out) set\n",
    "* Train on the training set, and test for accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training and test set.\n",
    "Xlr, Xtestlr, ylr, ytestlr = train_test_split(loans_df[['loan_amnt', 'credit_grade', 'int_rate', 'derived_income', 'derived_dti', 'inst_inc_ratio']].values,\n",
    "                                              (loans_df.loan_status_bin).values,\n",
    "                                              random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.686339561405\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "# Fit the model on the trainng data.\n",
    "clf.fit(Xlr, ylr)\n",
    "# Print the accuracy from the testing data.\n",
    "print(accuracy_score(clf.predict(Xtestlr), ytestlr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Model\n",
    "\n",
    "The model has some hyperparameters we can tune for hopefully better performance. For tuning the parameters of your model, you will use a mix of *cross-validation* and *grid search*. In Logistic Regression, the most important parameter to tune is the *regularization parameter* `C`. Note that the regularization parameter is not always part of the logistic regression model. \n",
    "\n",
    "The regularization parameter is used to control for unlikely high regression coefficients, and in other cases can be used when data is sparse, as a method of feature selection.\n",
    "\n",
    "You will now implement some code to perform model tuning and selecting the regularization parameter $C$.\n",
    "\n",
    "We use the following `cv_score` function to perform K-fold cross-validation and apply a scoring function to each test fold. In this incarnation we use accuracy score as the default scoring function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def cv_score(clf, x, y, score_func=accuracy_score):\n",
    "    result = 0\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(x): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y[train]) # fit\n",
    "        result += score_func(clf.predict(x[test]), y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of using the `cv_score` function for a basic logistic regression model without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression()\n",
    "score = cv_score(clf1, Xlr, ylr)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Checkup Exercise Set II</h3>\n",
    "\n",
    "<b>Exercise:</b> Implement the following search procedure to find a good model\n",
    "<ul>\n",
    "<li> You are given a list of possible values of `C` below\n",
    "<li> For each C:\n",
    "  <ol>\n",
    "  <li> Create a logistic regression model with that value of C\n",
    "  <li> Find the average score for this model using the `cv_score` function **only on the training set** `(Xlr, ylr)`\n",
    "  </ol>\n",
    "<li> Pick the C with the highest average score\n",
    "</ul>\n",
    "Your goal is to find the best model parameters based *only* on the training set, without showing the model test set at all (which is why the test set is also called a *hold-out* set).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the grid of parameters to search over\n",
    "Cs = [0.001, 0.1, 1, 10, 100]\n",
    "max_score=0\n",
    "\n",
    "for C in Cs:\n",
    "    clf2 = LogisticRegression(C=C)\n",
    "    score = cv_score(clf2, Xlr, ylr)\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        best_C =C\n",
    "print ('max_score: ',max_score, 'best_C: ', best_C)\n",
    "\n",
    "# your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Checkup Exercise Set III</h3>\n",
    "**Exercise:** Now you want to estimate how this model will predict on unseen data in the following way:\n",
    "<ol>\n",
    "<li> Use the C you obtained from the procedure earlier and train a Logistic Regression on the training data\n",
    "<li> Calculate the accuracy on the test data\n",
    "</ol>\n",
    "\n",
    "<p>You may notice that this particular value of `C` may or may not do as well as simply running the default model on a random train-test split. </p>\n",
    "\n",
    "<ul>\n",
    "<li> Do you think that's a problem? \n",
    "<li> Why do we need to do this whole cross-validation and grid search stuff anyway?\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3=LogisticRegression(C=best_C)\n",
    "clf3.fit(Xlr, ylr)\n",
    "ypred=clf3.predict(Xtestlr)\n",
    "print('accuracy score: ', accuracy_score(ypred, ytestlr), '\\n')\n",
    "print('I don\\'t think there is a problem, since model accuracy has '\n",
    "      'increased with addition of a regularization parameter')\n",
    "print('We perform cross-validation and grid search to tune hyperparameters of our model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Black Box Grid Search in `sklearn`\n",
    "\n",
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Checkup Exercise Set IV</h3>\n",
    "\n",
    "<b>Exercise:</b> Use scikit-learn's [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) tool to perform cross validation and grid search. \n",
    "\n",
    "* Instead of writing your own loops above to iterate over the model parameters, can you use GridSearchCV to find the best model over the training set? \n",
    "* Does it give you the same best value of `C`?\n",
    "* How does this model you've obtained perform on the test set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your turn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf4=LogisticRegression()\n",
    "parameters = {\"C\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "fitmodel = GridSearchCV(clf4, param_grid=parameters, cv=5, scoring=\"accuracy\", return_train_score=True)\n",
    "fitmodel.fit(Xlr, ylr)\n",
    "fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.cv_results_\n",
    "\n",
    "clf5=LogisticRegression(C=fitmodel.best_params_['C'])\n",
    "clf5.fit(Xlr, ylr)\n",
    "ypred=clf5.predict(Xtestlr)\n",
    "\n",
    "print('accuracy score: ', accuracy_score(ypred, ytestlr), '\\n')\n",
    "print('No, the new value of the C is: ', fitmodel.best_params_['C'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# fit a CART model to the data\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_dt.fit(Xlr, ylr)\n",
    "print(clf_dt)\n",
    "# make predictions\n",
    "ypred = clf_dt.predict(Xtestlr)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(ytestlr, ypred))\n",
    "print(metrics.confusion_matrix(ytestlr, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Sampling using SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Logistic Regression using SMOTE sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Current', 457983), ('Default', 457983), ('Fully Paid', 457983), ('Late', 457983)]\n"
     ]
    }
   ],
   "source": [
    "X_resampled, y_resampled = SMOTE().fit_sample(Xlr, ylr)\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Current', 152256), ('Default', 152256), ('Fully Paid', 152256), ('Late', 152256)]\n"
     ]
    }
   ],
   "source": [
    "X_test_resampled, y_test_resampled = SMOTE().fit_sample(Xtestlr, ytestlr)\n",
    "print(sorted(Counter(y_test_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "clf_smote = LogisticRegression().fit(X_resampled, y_resampled)\n",
    "print(clf_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Current       0.39      0.45      0.41    152256\n",
      "    Default       0.33      0.51      0.40    152256\n",
      " Fully Paid       0.36      0.23      0.28    152256\n",
      "       Late       0.29      0.19      0.23    152256\n",
      "\n",
      "avg / total       0.34      0.34      0.33    609024\n",
      "\n",
      "[[68098 42591 22300 19267]\n",
      " [24314 78263 21123 28556]\n",
      " [47090 47991 35390 21785]\n",
      " [36710 66505 20803 28238]]\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "ypred = clf_smote.predict(X_test_resampled)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(y_test_resampled, ypred))\n",
    "print(metrics.confusion_matrix(y_test_resampled, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.344795935792 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy score: ', accuracy_score(ypred, y_test_resampled), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Decision tree (CART) using SMOTE sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# fit a CART model to the data\n",
    "clf_dt_smote = DecisionTreeClassifier()\n",
    "clf_dt_smote.fit(X_resampled, y_resampled)\n",
    "print(clf_dt_smote)\n",
    "# make predictions\n",
    "ypred = clf_dt_smote.predict(X_test_resampled)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(y_test_resampled, ypred))\n",
    "print(metrics.confusion_matrix(y_test_resampled, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Random forest using SMOTE sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "clf_rf_1 = RandomForestClassifier(max_depth=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02463866  0.52023963  0.36261194  0.01070639  0.06651044  0.01529294]\n"
     ]
    }
   ],
   "source": [
    "clf_rf_1.fit(X_resampled, y_resampled)\n",
    "print(clf_rf_1.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(clf_rf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Current       0.58      0.66      0.62    152256\n",
      "    Default       0.40      0.43      0.41    152256\n",
      " Fully Paid       0.48      0.29      0.36    152256\n",
      "       Late       0.41      0.48      0.44    152256\n",
      "\n",
      "avg / total       0.47      0.46      0.46    609024\n",
      "\n",
      "[[100343  16404  13641  21868]\n",
      " [ 13631  64941  19660  54024]\n",
      " [ 41394  36803  44733  29326]\n",
      " [ 18456  45895  14915  72990]]\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "ypred = clf_rf_1.predict(X_test_resampled)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(y_test_resampled, ypred))\n",
    "print(metrics.confusion_matrix(y_test_resampled, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Random Forest - Attempt I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Current' 'Current' 'Fully Paid' ..., 'Late' 'Late' 'Late']\n"
     ]
    }
   ],
   "source": [
    "print(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "RANDOM_STATE = 123\n",
    "\n",
    "# Generate a binary classification dataset.\n",
    "X, y = X_resampled, y_resampled\n",
    "\n",
    "#Xtestlr, ytestlr\n",
    "\n",
    "# NOTE: Setting the `warm_start` construction parameter to `True` disables\n",
    "# support for parallelized ensembles but is necessary for tracking the OOB\n",
    "# error trajectory during training.\n",
    "ensemble_clfs = [\n",
    "    (\"RandomForestClassifier, max_features='sqrt'\",\n",
    "        RandomForestClassifier(warm_start=True, oob_score=True,\n",
    "                               max_features=\"sqrt\",\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RandomForestClassifier, max_features='log2'\",\n",
    "        RandomForestClassifier(warm_start=True, max_features='log2',\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RandomForestClassifier, max_features=None\",\n",
    "        RandomForestClassifier(warm_start=True, max_features=None,\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
    "error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 15\n",
    "max_estimators = 175\n",
    "\n",
    "for label, clf in ensemble_clfs:\n",
    "    for i in range(min_estimators, max_estimators + 1):\n",
    "        clf.set_params(n_estimators=i)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        # Record the OOB error for each `n_estimators=i` setting.\n",
    "        oob_error = 1 - clf.oob_score_\n",
    "        error_rate[label].append((i, oob_error))\n",
    "\n",
    "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
    "for label, clf_err in error_rate.items():\n",
    "    xs, ys = zip(*clf_err)\n",
    "    plt.plot(xs, ys, label=label)\n",
    "\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Random Forest - II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "clf_rf_2 = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = clf_rf_2, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rf_random.be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SVM on resampled data (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf_svm_smote = SVC()\n",
    "clf_svm_smote.fit(X_resampled, y_resampled)\n",
    "print(clf_svm_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "#ypred = clf_svm_smote.predict(X_test_resampled)\n",
    "# summarize the fit of the model\n",
    "#print(metrics.classification_report(y_test_resampled, ypred))\n",
    "#print(metrics.confusion_matrix(y_test_resampled, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Sampling using of ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = ADASYN().fit_sample(Xlr, ylr)\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_resampled, y_test_resampled = ADASYN().fit_sample(Xtestlr, ytestlr)\n",
    "print(sorted(Counter(y_test_resampled).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression using ADASYN sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_adasyn = LogisticRegression().fit(X_resampled, y_resampled)\n",
    "print(clf_adasyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "ypred = clf_adasyn.predict(X_test_resampled)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(y_test_resampled, ypred))\n",
    "print(metrics.confusion_matrix(y_test_resampled, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling using imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_sample(X_resampled, y_resampled)\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rus = LogisticRegression(C=0).fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_resampled, y_test_resampled = rus.fit_sample(Xtestlr, ytestlr)\n",
    "print(sorted(Counter(y_test_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "ypred = clf_smote.predict(X_test_resampled)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(y_test_resampled, ypred))\n",
    "print(metrics.confusion_matrix(y_test_resampled, ypred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
